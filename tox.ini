# This file provides configurations for tox-based project automation. Generally, this project uses tox similar to how
# some other projects use build-systems.

# Base tox configurations. Note, the 'envlist' will be run in the listed order whenever 'tox' is used without an -e
# specifier.
[tox]
requires = tox>=4
envlist =
    stubs
    lint
    {py310, py311, py312}-test
    combine-test-reports
    doxygen
    docs

# This forces tox to create a 'sterile' environment into which the project with all dependencies is installed prior to
# running the requested tasks, isolating the process from the rest of the system. This is almost always the desired
# runtime mode.
isolated_build = True

# Note: The 'basepython' argument should either be set to the oldest version in the supported stack or to the main
# version. It controls the specific ruleset used to format and (especially) style-check the code.
[testenv: lint]
description =
    Runs static code formatting, style and typing checkers. Type checker may not work properly until stubs are
    generated via the 'stubs' task. Nevertheless, the tasks are independent and can be run in any order.
basepython = py310
extras = lint
commands =
    ruff check --select I --fix
    ruff format
    mypy . --strict --extra-checks

# Note: Uses mypy v 1.4.0 Due to a persistent failure of 1.5.0 to work as intended (runs into a fatal error, cause
# not known). v 1.10 works fine for source documentation, but since we are using c-extensions and mypy does not
# process c-sources, the lesser of two evils was chosen and v 1.4.0 is used for now.
[testenv: stubs]
description =
    Ensures that the source code contains the py.typed marker at the top of the '/src' hierarchy. Then, generates
    stubs for all files inside the (distribution) package directory. Finally, moves the files to the same location,
    relative to the '/src' directory as they are found inside the distributed pacakge. Jointly, these
    steps make static type-checkers like mypy properly recognize and work with the installed pacakge.
deps = mypy==1.4.0
commands =
    python automation.py process-typed-markers
    python -m pip install .
    stubgen -o stubs --include-private -p ataraxis_time -v
    python automation.py process-stubs

[testenv: {py310, py311, py312}-test]
description =
    Runs unit and integration tests for each of the python versions listed in the environment name. The tests should be
    using the intended library name, as the project is compiled and installed as a library prior to running the tests.
    To optimize testing speed, defaults to using all logical cores of the host-platform and generates coverage reports.
    Uses 'loadgroup' balancing method to distribute the tests across all workers while allowing manually assigning some
    tests to use the same worker (see pytest-xdist for details).
package = wheel
extras = test
allowlist_externals = pytest
# Sets environment parameters, which includes intermediate coverage aggregation file used by combine-test-reports.
setenv =
    COVERAGE_FILE = reports{/}.coverage.{envname}
deps =
    scikit-build-core
    nanobind
    psutil
    pytest
commands =
    # Make sure the --cov is always set to the intended library name, so that coverage runs on the whole library
    # exactly once.
    python -m pip install .
    pytest \
    --import-mode=append \
    --cov=ataraxis_time \
    --cov-config=pyproject.toml \
    --cov-report=xml \
    --junitxml=reports/pytest.xml.{envname} \
    -n logical --dist loadgroup

[testenv:combine-test-reports]
description =
  Combines test and coverage data from multiple test runs into a single html file. The file can be viewed by loading
  the 'reports/coverage_html/index.html'.
skip_install = true
setenv = COVERAGE_FILE = reports/.coverage
depends = {py310, py311, py312}-test
deps =
    junitparser
    coverage[toml]
commands =
    junitparser merge --glob reports/pytest.xml.* reports/pytest.xml
    coverage combine --keep
    coverage xml
    coverage html

# Note: since doxygen is not pip-installable, it has to be installed and made available system-wide for this task to
# succeed. Consult https://www.doxygen.nl/manual/install.html for guidance.
[testenv:doxygen]
description =
    Generates C++ / C source code documentation using Doxygen. This assumes the source code uses doxygen-compatible
    docstrings and that the root directory contains a Doxyfile that minimally configures Doxygen runtime. This task
    is only needed for projects with C-extensions. This task works in tandem with the main 'docs' task.
allowlist_externals = doxygen
commands =
# Instructs doxygen to use the local Doxyfile instance to parse C++ docstrings
    doxygen Doxyfile


[testenv:docs]
description =
    Builds the API documentation from source code docstrings using Sphinx. Integrates with C / C++ documentation via
    Breathe, provided that doxygen was used to generate the initial .xml file for C-extension sources. The result can
    be viewed by loading 'docs/build/html/index.html'.
extras = docs
deps =
    importlib_metadata
    breathe
    sphinx-click
    sphinx-rtd-theme
allowlist_externals =
    sphinx-build
commands =
# Instructs the sphinx to build the html documentation using local configuration files. uses '-j auto' to parallelize
# the build process and '-v' to make it verbose.
    sphinx-build -b html -d docs/build/doctrees docs/source docs/build/html -j auto -v


[testenv:build]
description =
    Builds the source code distribution (sdist) and then uses 'cibuildwheel' to compile binary wheels. The wheels
    contain compiled c-extension code and pure-python code. Use 'upload' task to subsequently upload built wheels to
    PIP. Note, you can use the 'platform' flag to build linux (requires docker) wheels on non-native platforms.
extras = build
allowlist_externals =
    docker
    python
deps =
    scikit-build-core
    nanobind
    cibuildwheel
    build
commands =
    python -m build . --sdist
    cibuildwheel --output-dir dist --platform auto

[testenv:upload]
description =
    Uses twine to upload all files inside the '/dist' folder to pip, ignoring any files that might have been already
    uploaded. The first time this command runs on a new machine, this generates a private .pypirc file and asks for the
    upload token. Subsequently, the locally-stored token will be used each time this command is called.
deps =
    twine
allowlist_externals =
    python
commands =
    python automation.py set-pypi-token
    twine upload dist/* --skip-existing --config-file .pypirc --verbose

# Note: This task automatically uses the latest PIP version. Ideally, it should be used together with the build and
# twine task, as that would ensure the recipe always matches the latest source code version.
[testenv:recipe]
description =
    Uses grayskull to parse the source code tarball stored on pip and generate the recipe used to submit the
    package to conda-forge.
extras = upload
depends =
    build
    upload
allowlist_externals =
    python
    distutils
commands =
    python automation.py generate-recipe
    grayskull pypi ataraxis-time -o recipe --strict-conda-forge --list-missing-deps -m Inkaros

[testenv:export-env]
skip_install = True
deps = click
description =
    Exports the project's environment to the 'envs' folder as a .yml file and as a spec.txt with revision history.
    This process automatically determines the appropriate os-suffix and removes the prefix from the exported .yml file.
    Overall, this task should be called whenever teh development environment for the given platform is updated.
allowlist_externals =
    python
commands =
    python automation.py export-env --base-env axt_dev

[testenv:import-env]
skip_install = True
deps = click
description =
    Automatically resolves the appropriate environment os-suffix and attempts to import the first discovered .yml
    file with the appropriate os-suffix into local conda. If an environment with the same name as the imported
    environment already exists, it will be updated with the data extracted from the .yml file.
allowlist_externals =
    python
commands =
    python automation.py import-env
