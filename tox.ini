# This file provides configurations for various project-building tasks. In a way, Tox is used as a build system for
# this project. Note, all tasks listed in this file should be executed successfully prior to merging ANY changes into
# the main branch.

# Configures the 'general' runtime commands. This allows specifying the list of commands to be executed when 'tox'
# command is used without any environment specification. In turn, this configuration allows using plain 'tox' call to
# carry out all project checkout tasks with a single call.
[tox]
requires = tox>=4

envlist =
    lint
    stubs
    {py310, py311, py312}-test
    combine-test-reports
    doxygen
    docs

# This forces tox to create a 'sterile' environment into which the project with all dependencies is installed prior to
# running the requested tasks, isolating the process from the rest of the system. This is almost always the desired
# runtime mode.
isolated_build = True

# Note: The 'basepython' argument should either be set to the oldest version in the supported stack or to the main
# version. It controls the specific ruleset used to format and (especially) style-check the code.
[testenv: lint]
description =
    Runs static code formatting, style and typing checkers. Type checker may not work properly until stubs are generated
    via the 'stubs' task.
basepython = py310
extras = lint
commands =
    ruff check --select I --fix
    ruff format
    mypy . --strict --extra-checks

# Note: For small codebases, setting the workers to a small number is likely to yield better results. For big projects,
# using 'auto' or high worker counts for the automation script can yield better processing speeds by parallelizing
# moving stubs and adding py.typed markers.
[testenv: stubs]
description =
    Generates or regenerates stubs for the source code and moves them to destination locations (to the same level as
    each .py source file). Also ensures that each independent python package hierarchy in the /src directory has a
    py.typed at the highest level.
deps = mypy
commands =
    python -m pip install .
    stubgen -o stubs --include-private -p ataraxis_time
    #python automation.py process-stubs

# Main testing task. This task builds and installs the package and then runs the contents of the 'tests' folder using
# pytest framework on the installed package. This ensures the package works as intended for the target python versions.
# Note, you can specify the particular python version to use during runtime, if you only need to test one of them at a
# time. This task requires the target Python to be installed on the host system and, by far, the easiest solution is to
# install the standalone python version(s) you need for testing.
[testenv: {py310, py311, py312}-test]
description = Runs unit and integration tests using parallel workers and generates coverage reports.
package = wheel
extras = test
allowlist_externals = pytest
# Sets environment parameters, which includes the coverage_report parameters.
setenv =
    COVERAGE_FILE = reports{/}.coverage.{envname}
deps =
    scikit-build-core
    nanobind
    psutil
    pytest
commands =
# Builds and installs the package and runs pytest-cov. The coverage reports are aggregated into the unified
# html file to be reviewed by the developers at the end of the testing runtime. To optimize testing speed, the tests
# run in parallel via pytest-xdist plugin. The '-n auto' allows the plugin to automatically determine the number of
# workers, but it an also be configured to a specific number.
    python -m pip install .
    pytest \
    --import-mode=append \
    --cov=ataraxis_time \
    --cov-config=pyproject.toml \
    --cov-report=xml \
    --junitxml=reports/pytest.xml.{envname} \
    -n logical --dist loadgroup

# A sub-task executed for each '-test' call. This task aggregates the coverage reports for different python versions
# into an .html file to be reviewed by the developers. To view the report, navigate to reports/coverage_html/index.html.
[testenv:combine-test-reports]
description = Combines test and coverage data from multiple test runs into a file.
skip_install = true
setenv = COVERAGE_FILE = reports/.coverage
depends = {py310, py311, py312}-test
deps =
    junitparser
    coverage[toml]
commands =
# Uses junitparser to read the pytest .xml reports and convert them to a unified html summary file
    junitparser merge --glob reports/pytest.xml.* reports/pytest.xml
    coverage combine --keep
    coverage xml
    coverage html

# This task is only required for project that use C / C++ extensions. To unify the documentation for the project, this
# task uses doxygen to generate the API documentation using doxygen-formatted docstrings and outputs it as an .xml,
# which is then used by 'breathe' to generate sphinx-compatible files. This step is necessary to generate the original
# C / C++ source documentation. Note, since doxygen is not pip-installable, it has to be installed and made available
# system-wide for this task to succeed. Consult https://www.doxygen.nl/manual/install.html for guidance.
[testenv:doxygen]
description = Generates Doxygen documentation.
allowlist_externals = doxygen
commands =
# Instructs doxygen to use the local Doxyfile instance to parse C++ docstrings
    doxygen Doxyfile

# Builds the read-the-docs formatted API documentation html page via sphinx. Sphinx is configured to build the Python
# documentation using Google-style docstrings and to build C++ documentation via breathe linker (from doxygen .xml).
# This task works in-tandem with conf.py and the .rst files inside the docs/source directory.
[testenv:docs]
description = Builds the API documentation using Sphinx (integrates with C++ documentation via Breathe).
extras = docs
deps =
    importlib_metadata
    breathe
    sphinx-click
    sphinx-rtd-theme
allowlist_externals =
    sphinx-build
commands =
# Instructs the sphinx to build the html documentation using local configuration files. uses '-j auto' to parallelize
# the build process and '-v' to make it verbose.
    sphinx-build -b html -d docs/build/doctrees docs/source docs/build/html -j auto -v

# Another C++ / C extension exclusive task. This task assembles the project wheels, which includes compiling the
# OS- and platform-specific version of the C++ extension. This task should be used on all supported systems (virtual or
# physical) to generate the wheels to be used for end-user distribution. Note, this task is not part of the 'default'
# tox command and has to be called manually, as building the wheels is an intensive process that is only required for
# the release versions of the project (unlike other development steps that, ideally, should be run before any change is
# pushed back to GitHub). The wheels need to be manually uploaded to pip via 'twine' and to conda.
[testenv:build]
description = Build the package.
extras = build
allowlist_externals =
    docker
    python
deps =
    scikit-build-core
    nanobind
    cibuildwheel
    build
commands =
# Builds the source distribution in addition to wheels.
    python -m build . --sdist
# cibuildwheel --output-dir dist --platform linux

[testenv:upload]
description =
    Uses twine to upload all files inside the /dist folder to pip, ignoring any files that might
deps =
    twine
allowlist_externals =
    python
commands =
    python automation.py set-pypi-token
    twine upload dist/* --skip-existing --config-file .pypirc

# Note: This task automatically uses the latest PIP version. Ideally, it should be used together with the build and
# twine task, as that would ensure the recipe always matches the latest source code version.
[testenv:recipe]
description =
    Uses grayskull to parse the source code tarball stored on pip and generate the recipe used to submit the
    package to conda-forge.
depends =
    build
    upload
deps =
    grayskull
    twine
allowlist_externals =
    python
    distutils
commands =
    python automation.py generate-recipe
    grayskull pypi ataraxis-time -o recipe --strict-conda-forge --list-missing-deps -m Inkaros
